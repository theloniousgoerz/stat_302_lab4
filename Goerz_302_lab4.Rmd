---
title: "Lab 4"
author: "STAT 302"
date: "5/29/21"
output: html_document
---

<!--- Begin styling code. --->
<style type="text/css">
/* Whole document: */
body{
  font-family: "Palatino Linotype", "Book Antiqua", Palatino, serif;
  font-size: 12pt;
}
h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author {
  font-size: 18px;
  text-align: center;
}
h4.date {
  font-size: 18px;
  text-align: center;
}
</style>
<!--- End styling code. --->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 100)
```

*If you collaborated with anyone, you must include "Collaborated with: FIRSTNAME LASTNAME" at the top of your lab!*

For this lab, note that there are tidyverse methods to perform cross-validation in R (see the `rsample` package). However, your goal is to understand and be able to implement the algorithm "by hand", meaning  that automated procedures from the `rsample` package, or similar packages, will not be accepted.

To begin, load in the popular `penguins` data set from the package `palmerpenguins`.

```{r}
# install.packages("palmerpenguins")
library(palmerpenguins)
library(class)
library(tidyverse)
data(package = "palmerpenguins")
# Assign to an objsect
penguin_data <- penguins
```

## Part 1. k-Nearest Neighbors Cross-Validation (10 points)

Our goal here is to predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`.
All your code should be within a function `my_knn_cv`.

**Input:**

  * `train`: input data frame
  * `cl`: true class value of your training data
  * `k_nn`: integer representing the number of neighbors
  * `k_cv`: integer representing the number of folds
  
*Please note the distinction between `k_nn` and `k_cv`!*

**Output:** a list with objects

  * `class`: a vector of the predicted class $\hat{Y}_{i}$ for all observations
  * `cv_err`: a numeric with the cross-validation misclassification error


You will need to include the following steps:

* Within your function, define a variable `fold` that randomly assigns observations to folds $1,\ldots,k$ with equal probability. (*Hint: see the example code on the slides for k-fold cross validation*)
* Iterate through $i = 1:k$. 
  * Within each iteration, use `knn()` from the `class` package to predict the class of the $i$th fold using all other folds as the training data.
  * Also within each iteration, record the prediction and the misclassification rate (a value between 0 and 1 representing the proportion of observations that were classified **incorrectly**).
* After you have done the above steps for all $k$ iterations, store the vector `class` as the output of `knn()` with the full data as both the training and the test data, and the value `cv_error` as the average misclassification rate from your cross validation.

**Submission:** To prove your function works, apply it to the `penguins` data. Predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Use $5$-fold cross validation (`k_cv = 5`). Use a table to show the `cv_err` values for 1-nearest neighbor and 5-nearest neighbors (`k_nn = 1` and `k_nn = 5`). Comment on which value had lower CV misclassification error and which had lower training set error (compare your output `class` to the true class, `penguins$species`).

```{r}
set.seed(1)
#======
# Test the KNN function. 
# Create training and test data ====================================
# Clean the data for NA 
penguin_knn <- penguin_data %>% na.omit
# create sample
# Create a data frame of the input variables 
penguin_knn <- penguin_data %>% select(bill_depth_mm,bill_length_mm,body_mass_g,flipper_length_mm,species) %>% 
  na.omit
# Create a classifier for training and test data. 
# Merge that with the full data
type_t_t <- sample(rep(1:2, length = nrow(penguin_knn)))
penguin_knn <- penguin_knn %>% mutate(type = type_t_t)
# create a training data set 
penguin_train <- penguin_knn %>% filter(type == 1) %>% select(-type,-species)
# create a test data set 
penguin_test <- penguin_knn %>% filter(type == 2) %>% select(-type,-species)

# Create a test cl dataset which has the true Y values for the test set. 
penguin_test_cl <-  penguin_knn %>% filter(type == 2) %>% select(species)
# Make into a vector 
penguin_test_cl <- as_vector(penguin_test_cl)

#==================================================
#CL For Y 
# Create a factor of the output variables for Train 
penguin_knn_true_cl <- penguin_knn %>% filter(type == 1) %>% select(species)
# Make it a vector. 
penguin_knn_true_cl <- as_vector(penguin_knn_true_cl)



```

```{r}
# Replicate the KNN algorithm by hand. 
# This function "my_knn_cv" combines two other functions for convenience. 
# install.packages("class")
my_knn_cv <- function(train,cl,k_nn,k_cv) {
  # Depends on:
  require(class)
  require(tidyverse)
  # Create fold arg, with k partitions. 
  fold = sample(rep(1:k_cv,length = nrow(train)))
  # Add train to fold vector. 
  data = cbind(train,fold)
  # Create one with class
  data_w_cl = cbind(fold,cl)
  # Create full data for eventual KNN calc. 
  # Clone CL. 
  cl_full <- as_vector(cl)
  # Clone train. 
  train_full <- train
  # Create a list object for later values. 
    knn_list = list()
    knn_error_list = list()
    knn_corr_list = list()
  # Iterate through the Ks.
  for (i in 1:k_cv) { 
    # Create trainind and test data. 
    data_train = data %>% filter(fold != i) %>% 
      select(-fold)
    data_test = data %>% filter(fold == i) %>% 
      select(-fold)
    cl = data_w_cl %>% filter(fold != i) %>% 
      select(-fold)
    # Create a prediction vector. 
    cl_predict = data_w_cl %>% filter(fold == i) %>% 
      select(-fold)
    # Run KNN. 
    # Coerce type to work in the KNN function. 
    cl = as_vector(cl)
    cl_predict = as_vector(cl_predict)
   knn_iter <-  knn(data_train,
        data_test,
        cl = cl, 
        k = k_nn)
   # Calculate the correctly classified 
  knn_test_eq = knn_iter == cl_predict
  # Calculate the number of TRUEs
  num_corr_class = sum(knn_test_eq)
  # Calculate the correct classification rate 
  pct_corr_class = num_corr_class / length(knn_iter)
  # Calculate the missclassification 
  pct_miss_class = 1-pct_corr_class
  # Store these in the list 
  knn_corr_list[[i]] <- pct_corr_class
  knn_list[[paste("pred_class",i,sep = "_")]] <- knn_iter
  knn_error_list[[paste("pct_miss_class",i,sep = "_")]] <- pct_miss_class 
  }
  # Calculate return values. 
  # Calculate the mean missclassification
  # calculate the prediction for KNN 
  # Predict the final classification. 
    knn_prediction = knn(train_full,
                         train_full,
                         cl = cl_full, k = 5)
    ## Returns a KNN predictd off of all the test data. 
    # Calculate CV Error. 
cv_err = mean(unlist(knn_error_list))
# Create an output list 
 list_out = list()
 # Assign the values to a list. 
  list_out[["class"]] <- knn_prediction
  list_out[["cv_err"]] <- cv_err
  return(list_out)
  # Returns a list of the models and their predictions 
  # Returns the cv error 
}

# === Test function 
penguin_input_data <- penguin_knn %>% select(-type,-species) 
penguin_data_cl <- penguin_knn %>% select(species)

set.seed(1)
# Run k fold cross validation for 5NN. 
# Call the function 
# Input train 
penguin_k_fold_5 <- my_knn_cv(penguin_input_data,
          # Input real class values. 
          penguin_data_cl,
          # Select nearest neighbors. 
          k_nn = 5,
          # Select folds. 
          k_cv = 5)
# Run K fold for 1 NN. 
# Call the function 
# Input train 
penguin_k_fold_1 <- my_knn_cv(penguin_input_data,
          # Input real class values. 
          penguin_data_cl,
          # Select nearest neighbors. 
          k_nn = 1,
          # Select folds. 
          k_cv = 5)
# Create a table to compare the two cross validation errors. 
tibble(cv_error = c(penguin_k_fold_1$cv_err,penguin_k_fold_5$cv_err),
       nn = c(1,5))

## 
# Now compare the classification to the true data. 
# Compare real class with k 1 
Penguin_k1_compare <- penguin_k_fold_1$class == as_vector(penguin_data_cl)
Penguin_k1_compare
# Calculate the percent correctly classified. 
sum(Penguin_k1_compare) / length(Penguin_k1_compare)

# Compare with real class with k 5 
Penguin_k5_compare <- penguin_k_fold_5$class == as_vector(penguin_data_cl)
Penguin_k5_compare
# Calculate the percent correctly classified. 
sum(Penguin_k5_compare) / length(Penguin_k5_compare)
```

In these two models, the $\ k =1$ model had the lower cv misspecification error. 
Practically speaking, the $\ k=5$ model had the better predictive error but only marginally. 

## Part 2. Random Forest Cross-Validation (10 points)

Now, we will predict output `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`.
All your code should be within a function `my_rf_cv`.

**Input:**

  * `k`: number of folds

**Output:**

  * a numeric with the cross-validation error
  
Your code will look very similar to Part 1! You will need the following steps: 

* Within your function, define a variable `fold` within the `penguins` data that randomly assigns observations to folds $1,\ldots,k$ with equal probability. (*Hint: see the example code on the slides for k-fold cross validation*)
* Iterate through $i = 1:k$. 
  * Within each iteration, define your training data as all the data not in the $i$th fold.
  * Also within each iteration, use `randomForest()` from the `randomForest` package to train a random forest model with $100$ trees to predict `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. <br>
*Hint: `randomForest()` takes formula input. Your code here will probably look something like: *
`MODEL <- randomForest(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, data = TRAINING_DATA, ntree = 100)`
  * Also within each iteration, predict the `body_mass_g` of the $i$th fold which was not used as training data. <br>
  *Hint: predicting with `randomForest()` works similar to `lm()`. Your code here will probably looks something like: *
  `PREDICTIONS <- predict(MODEL, TEST_DATA[, -1])`
  *where we remove the first column, `body_mass_g` from our test data.*
  * Also within each iteration, evaluate the MSE, the average squared difference between predicted `body_mass_g` and true `body_mass_g`.
* Return the average MSE across all $k$ folds.

**Submission:** 
To prove your function works, apply it to the `penguins` data. Predict `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`.
Run your function with $5$-fold cross validation (`k = 5`) and report the CV MSE.


```{r}
# Clean data for testing random forest. 
penguin_data <- penguins %>% na.omit
# ================================
# Begin function. 
#install.packages("randomForest")
# install.packages("randomForest")
library(randomForest)
# RF cross validation 
# Penguin data random forest cross validation. 
my_rf_cv <- function(k) {
  # Require packages for function. 
  require(palmerpenguins)
  require(tidyverse)
  require(randomForest)
  # Define data. 
  data = penguins %>% 
    # Clean data for NA
    na.omit() %>% 
    # Define data input. 
    # Select Vars 
    select(flipper_length_mm, 
           bill_length_mm,
           bill_depth_mm,
           body_mass_g)
  # Define fold. 
  fold = sample(rep(1:k,length = nrow(data)))
  data = cbind(data, fold)
  # Create a list for the data 
  rf_list = list()
  mse_list = list()
  for (i in 1:k) {
    # Define data 
    data_train = data %>% 
      # Filter folds to all but i 
      filter(fold != i) %>% 
      # Rm fold from computation.
      select(-fold)
    # Filter test data to i. 
    data_test = data %>% filter(fold == i) 
    # Define the y outcome to calculate against
    data_test_y = data_test$body_mass_g
    # Remove fold from data test.
    data_test = data_test %>% select(-fold)
    # run random forest model 
    rand_for <- randomForest(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, 
                             # Data.
                 data = data_train,
                 # Set ntree to 100
                 ntree = 100)
    # Compute prediction on test data. 
   prediction <-  predict(rand_for, newdata = data_test)
   # Assign random forest estimates to a list. 
    rf_list[[i]] <- rand_for
    # Compute MSE 
    # Compute MSE 
    mse = mean((prediction - data_test_y)^2)
    # Add MSE to a list. 
    mse_list[[i]] <- mse
  }
  # Compute cross validation error. 
  cv_err = mean(unlist(mse_list))
  # Return Cross validation error. 
  return(cv_err)
  # Returns the numeric CV error. 
}
# Test function 
# ====================
cv_err_rf <- my_rf_cv(k = 5)
# Table CV err for simulation. 
data_frame(Cv_err = cv_err_rf)
```

The mean squared error of the five fold cross validation was `r round(cv_err_rf,3)`. 

